{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myRR-cUHQYWg",
        "outputId": "bab6925b-3802-4e33-d871-9035a5000d59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-30 01:59:18--  https://github.com/Prayas-Agrawal/tox-bias-data/raw/main/data.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Prayas-Agrawal/tox-bias-data/main/data.zip [following]\n",
            "--2023-11-30 01:59:18--  https://raw.githubusercontent.com/Prayas-Agrawal/tox-bias-data/main/data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99173537 (95M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]  94.58M   304MB/s    in 0.3s    \n",
            "\n",
            "2023-11-30 01:59:20 (304 MB/s) - ‘data.zip’ saved [99173537/99173537]\n",
            "\n",
            "Collecting shap\n",
            "  Downloading shap-0.43.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (532 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.9/532.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.2)\n",
            "Collecting slicer==0.0.7 (from shap)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.43.0 slicer-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/Prayas-Agrawal/tox-bias-data/raw/main/data.zip\n",
        "!pip install shap\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FvuOfb09Tql",
        "outputId": "e48d8706-359d-4092-997a-719f815804cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/data.zip\n",
            "  inflating: toxicity_annotated_comments.tsv  \n",
            "  inflating: toxicity_annotations.tsv  \n",
            "  inflating: toxicity_worker_demographics.tsv  \n",
            "  inflating: wiki_dev.csv            \n",
            "  inflating: wiki_dev_sep.txt        \n",
            "  inflating: wiki_noised_dev.csv     \n",
            "  inflating: wiki_noised_test.csv    \n",
            "  inflating: wiki_noised_train.csv   \n",
            "  inflating: wiki_test.csv           \n",
            "  inflating: wiki_test_sep.txt       \n",
            "  inflating: wiki_train.csv          \n",
            "  inflating: wiki_train_sep.txt      \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/data.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHeM5OrdXXyh",
        "outputId": "e7d42ee4-7418-4cf4-844f-4f4086cd4282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-30 01:59:33--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-11-30 01:59:33--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-11-30 01:59:34--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2023-11-30 02:02:13 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ze8L5vF7i5b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDOZNe3o9ypU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import tensorflow.compat.v1 as tf\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb\n",
        "\n",
        "\n",
        "UNK = '<unk>'\n",
        "PAD = '<pad>'\n",
        "TARGET = '<target>'\n",
        "\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for all datasets except for SST.\n",
        "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()\n",
        "\n",
        "def load_data_and_labels(data_file, sent_filter=[]):\n",
        "    \"\"\"\n",
        "    Loads data from files, splits the data into words and generates labels.\n",
        "    Returns split sentences and labels.\n",
        "    \"\"\"\n",
        "    # Load data from files\n",
        "    examples = list(open(data_file, \"r\", encoding='utf-8').readlines())\n",
        "    x_text = []\n",
        "    labels = []\n",
        "    for i, s in enumerate(examples):\n",
        "      split = s.strip().split(\"|||\")\n",
        "      label = split[0]\n",
        "      text = \"\".join(split[1:])\n",
        "      cleaned_str = clean_str(text)\n",
        "      # Only include words in sent_filter\n",
        "      if len(sent_filter) > 0 and set(sent_filter).isdisjoint(cleaned_str.split()):\n",
        "        continue\n",
        "      x_text.append(clean_str(text))\n",
        "      labels.append(int(label))\n",
        "    y = np.array(labels)\n",
        "    return x_text, y\n",
        "\n",
        "def get_word_count(corpus):\n",
        "  word_count = {}\n",
        "  for sentence in corpus:\n",
        "    words = sentence.split()\n",
        "    for word in words:\n",
        "      if word not in word_count:\n",
        "        word_count[word] = 0\n",
        "      word_count[word] += 1\n",
        "  return word_count\n",
        "\n",
        "def write_list_to_file(l, filename):\n",
        "  with open(filename, 'w') as f:\n",
        "    for item in l:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "\n",
        "def build_vocab(data_path, min_freq, output_path, target_words=None):\n",
        "  data = list(open(data_path, \"r\", encoding='utf-8').readlines())\n",
        "  text = [clean_str(line.strip().split(\"|||\")[1]) for line in data]\n",
        "  word_count = get_word_count(text)\n",
        "  idx2word = [UNK, PAD]\n",
        "  word2idx = {UNK : 0, PAD : 1}\n",
        "  if target_words:\n",
        "    idx2word.append(TARGET)\n",
        "    word2idx[TARGET] = 3\n",
        "  for word in word_count:\n",
        "    if target_words and word in target_words:\n",
        "      continue\n",
        "    if word_count[word] >= min_freq:\n",
        "      idx2word.append(word)\n",
        "      word2idx[word] = len(idx2word) - 1\n",
        "\n",
        "  write_list_to_file(idx2word, output_path)\n",
        "  return idx2word, word2idx\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "  with open(vocab_file) as f:\n",
        "    lines = f.readlines()\n",
        "  idx2word = []\n",
        "  word2idx = {}\n",
        "  for i, line in enumerate(lines):\n",
        "    vocab = line.strip()\n",
        "    idx2word.append(vocab)\n",
        "    word2idx[vocab] = i\n",
        "  return idx2word, word2idx\n",
        "\n",
        "def text2idx(text, word2idx, max_len, target_words=[]):\n",
        "  ret = []\n",
        "  for sentence in text:\n",
        "    words = sentence.split()\n",
        "    idx = []\n",
        "    for i, word in enumerate(words):\n",
        "      if len(idx) >= max_len:\n",
        "        break\n",
        "      if word in word2idx:\n",
        "        idx.append(word2idx[word])\n",
        "      elif word in target_words:\n",
        "        idx.append(word2idx[TARGET])\n",
        "      else:\n",
        "        idx.append(word2idx[UNK])\n",
        "    for i in range(len(idx), max_len):\n",
        "      idx.append(word2idx[PAD])\n",
        "    ret.append(idx)\n",
        "  return np.array(ret)\n",
        "\n",
        "def get_all_pad(length, word2idx):\n",
        "  return np.array([[word2idx[PAD] for i in range(length)]])\n",
        "\n",
        "def batch_iter(inputs, outputs, batch_size, num_epochs, shuffle=False, seed=None):\n",
        "    if seed is not None:\n",
        "      np.random.seed(seed)\n",
        "    inputs = np.array(inputs)\n",
        "    outputs = np.array(outputs)\n",
        "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
        "    for epoch in range(num_epochs):\n",
        "      if shuffle:\n",
        "        p = np.random.permutation(len(inputs))\n",
        "        inputs = inputs[p]\n",
        "        outputs = outputs[p]\n",
        "      for batch_num in range(num_batches_per_epoch):\n",
        "          start_index = batch_num * batch_size\n",
        "          end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
        "          yield inputs[start_index:end_index], outputs[start_index:end_index]\n",
        "\n",
        "def batch_iter_attr(inputs, outputs, attr_target, target_words_mask, batch_size, num_epochs, shuffle=False, seed=None):\n",
        "    if seed is not None:\n",
        "      np.random.seed(seed)\n",
        "    inputs = np.array(inputs)\n",
        "    outputs = np.array(outputs)\n",
        "    attr_target = np.array(attr_target)\n",
        "    target_words_mask = np.array(target_words_mask)\n",
        "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
        "    for epoch in range(num_epochs):\n",
        "      if shuffle:\n",
        "        p = np.random.permutation(len(inputs))\n",
        "        inputs = inputs[p]\n",
        "        outputs = outputs[p]\n",
        "        attr_target = attr_target[p]\n",
        "        target_words_mask = target_words_mask[p]\n",
        "      for batch_num in range(num_batches_per_epoch):\n",
        "          start_index = batch_num * batch_size\n",
        "          end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
        "          yield inputs[start_index:end_index], outputs[start_index:end_index], attr_target[start_index:end_index], target_words_mask[start_index:end_index]\n",
        "\n",
        "def preprocess(data_file, max_len, word2idx, target_words=None, return_attr_target=False, sent_filter=[], target_words_to_token=False):\n",
        "  x_text, y = load_data_and_labels(data_file, sent_filter)\n",
        "  if target_words_to_token and target_words:\n",
        "    x = text2idx(x_text, word2idx, max_len, target_words)\n",
        "  else:\n",
        "    x = text2idx(x_text, word2idx, max_len)\n",
        "  if target_words is not None and return_attr_target:\n",
        "    target_idx = [word2idx[word] for word in target_words if word in word2idx]\n",
        "    attr_target = np.zeros(x.shape)\n",
        "    attr_target[np.isin(x, target_idx)] = 1.0\n",
        "    attr_target = -1 * (attr_target - 1)\n",
        "    return x, y, attr_target\n",
        "  return x, y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdOMkCIIQcZi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "374982d1-cd75-4c20-fe15-9b00c51cff71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HELLO from model_tool\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Train a Toxicity model using Keras.\"\"\"\n",
        "\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import tensorflow.compat.v1 as tf\n",
        "# import data_utils as utils\n",
        "\n",
        "print('HELLO from model_tool')\n",
        "\n",
        "DEFAULT_EMBEDDINGS_PATH = './glove.6B.100d.txt'\n",
        "DEFAULT_MODEL_DIR = './'\n",
        "\n",
        "DEFAULT_HPARAMS = {\n",
        "    'max_sequence_length': 250,\n",
        "    'max_num_words': 10000,\n",
        "    'embedding_dim': 100,\n",
        "    \"seq_len\": 10,\n",
        "    'embedding_trainable': False,\n",
        "    'learning_rate': 0.00005,\n",
        "    'stop_early': True,\n",
        "    'es_patience': 1,  # Only relevant if STOP_EARLY = True\n",
        "    'es_min_delta': 0,  # Only relevant if STOP_EARLY = True\n",
        "    'batch_size': 128,\n",
        "    'epochs': 20,\n",
        "    'dropout_rate': 0.3,\n",
        "    'cnn_filter_sizes': [128, 128, 128],\n",
        "    'cnn_kernel_sizes': [5, 5, 5],\n",
        "    'cnn_pooling_sizes': [5, 5, 40],\n",
        "    'verbose': True\n",
        "}\n",
        "\n",
        "\n",
        "def compute_auc(y_true, y_pred):\n",
        "  try:\n",
        "    return metrics.roc_auc_score(y_true, y_pred)\n",
        "  except ValueError:\n",
        "    return np.nan\n",
        "\n",
        "### Model scoring\n",
        "\n",
        "\n",
        "# Scoring these dataset for dozens of models actually takes non-trivial amounts\n",
        "# of time, so we save the results as a CSV. The resulting CSV includes all the\n",
        "# columns of the original dataset, and in addition has columns for each model,\n",
        "# containing the model's scores.\n",
        "def score_dataset(df, models, text_col):\n",
        "  \"\"\"Scores the dataset with each model and adds the scores as new columns.\"\"\"\n",
        "  for model in models:\n",
        "    name = model.get_model_name()\n",
        "    print('{} Scoring with {}...'.format(datetime.datetime.now(), name))\n",
        "    df[name] = model.predict(df[text_col])\n",
        "\n",
        "\n",
        "def load_maybe_score(models, orig_path, scored_path, postprocess_fn):\n",
        "  \"\"\"Return dataset specified by the given path and cache it with its scores.\"\"\"\n",
        "  if os.path.exists(scored_path):\n",
        "    print('Using previously scored data:', scored_path)\n",
        "    return pd.read_csv(scored_path)\n",
        "\n",
        "  dataset = pd.read_csv(orig_path)\n",
        "  postprocess_fn(dataset)\n",
        "  score_dataset(dataset, models, 'text')\n",
        "  print('Saving scores to:', scored_path)\n",
        "  dataset.to_csv(scored_path)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def postprocess_madlibs(madlibs):\n",
        "  \"\"\"Modify madlibs data to have standard 'text' and 'label' columns.\"\"\"\n",
        "  # Native madlibs data uses 'Label' column with values 'BAD' and 'NOT_BAD'.\n",
        "  # Replace with a bool.\n",
        "  madlibs['label'] = madlibs['Label'] == 'BAD'\n",
        "  madlibs.drop('Label', axis=1, inplace=True)\n",
        "  madlibs.rename(columns={'Text': 'text'}, inplace=True)\n",
        "\n",
        "\n",
        "def postprocess_wiki_dataset(wiki_data):\n",
        "  \"\"\"Modify Wikipedia dataset to have 'text' and 'label' columns.\"\"\"\n",
        "  wiki_data.rename(\n",
        "      columns={\n",
        "          'is_toxic': 'label',\n",
        "          'comment': 'text'\n",
        "      }, inplace=True)\n",
        "\n",
        "\n",
        "class ToxModel():\n",
        "  \"\"\"Toxicity model.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               model_name=None,\n",
        "               model_dir=DEFAULT_MODEL_DIR,\n",
        "               embeddings_path=DEFAULT_EMBEDDINGS_PATH,\n",
        "               hparams=None,\n",
        "               typeArch=\"cnn\",\n",
        "               subtype=\"\"):\n",
        "    self.model_dir = model_dir\n",
        "    self.embeddings_path = embeddings_path\n",
        "    self.model_name = model_name\n",
        "    self.model = None\n",
        "    self.tokenizer = None\n",
        "    self.embedding_matrix = None\n",
        "    self.hparams = DEFAULT_HPARAMS.copy()\n",
        "    self.typeArch = typeArch\n",
        "    self.subtype = subtype\n",
        "    self.num_classes = 2\n",
        "    self.tokenize = False if self.typeArch == \"cnn_attr\" else True\n",
        "    self.seq_len = DEFAULT_HPARAMS['seq_len']\n",
        "    self.eager = False if self.typeArch == \"cnn_attr\" else True\n",
        "    self.batch_size = self.hparams['batch_size']\n",
        "    if hparams:\n",
        "      self.update_hparams(hparams)\n",
        "    if model_name:\n",
        "      self.load_model_from_name(model_name)\n",
        "    self.print_hparams()\n",
        "\n",
        "  def print_hparams(self):\n",
        "    print('Hyperparameters')\n",
        "    print('---------------')\n",
        "    for k, v in self.hparams.items():\n",
        "      print('{}: {}'.format(k, v))\n",
        "    print('')\n",
        "\n",
        "  def update_hparams(self, new_hparams):\n",
        "    self.hparams.update(new_hparams)\n",
        "\n",
        "  def get_model_name(self):\n",
        "    return self.model_name\n",
        "\n",
        "  def save_hparams(self, model_name):\n",
        "    self.hparams['model_name'] = model_name\n",
        "    with open(\n",
        "        os.path.join(self.model_dir, '%s_hparams.json' % self.model_name),\n",
        "        'w') as f:\n",
        "      json.dump(self.hparams, f, sort_keys=True)\n",
        "\n",
        "  def load_model_from_name(self, model_name):\n",
        "    \"\"\"Load model given its name.\"\"\"\n",
        "    self.model = tf.keras.models.load_model(\n",
        "        os.path.join(self.model_dir, '%s_model.h5' % model_name))\n",
        "    self.tokenizer = pickle.load(\n",
        "        open(\n",
        "            os.path.join(self.model_dir, '%s_tokenizer.pkl' % model_name),\n",
        "            'rb'))\n",
        "    with open(\n",
        "        os.path.join(self.model_dir, '%s_hparams.json' % self.model_name),\n",
        "        'r') as f:\n",
        "      self.hparams = json.load(f)\n",
        "\n",
        "  def fit_and_save_tokenizer(self, texts):\n",
        "    \"\"\"Fits tokenizer on texts and pickles the tokenizer state.\"\"\"\n",
        "    self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=self.hparams['max_num_words'])\n",
        "    self.tokenizer.fit_on_texts(texts)\n",
        "    pickle.dump(\n",
        "        self.tokenizer,\n",
        "        open(\n",
        "            os.path.join(self.model_dir, '%s_tokenizer.pkl' % self.model_name),\n",
        "            'wb'))\n",
        "\n",
        "  def prep_text(self, texts):\n",
        "    \"\"\"Turns text into into padded sequences.\n",
        "\n",
        "    The tokenizer must be initialized before calling this method.\n",
        "\n",
        "    Args:\n",
        "      texts: Sequence of text strings.\n",
        "\n",
        "    Returns:\n",
        "      A tokenized and padded text sequence as a model input.\n",
        "    \"\"\"\n",
        "    text_sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "    return tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        text_sequences, maxlen=self.hparams['max_sequence_length'])\n",
        "\n",
        "  def load_embeddings(self):\n",
        "    \"\"\"Loads word embeddings.\"\"\"\n",
        "    embeddings_index = {}\n",
        "    with open(self.embeddings_path) as f:\n",
        "      for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "    self.embedding_matrix = np.zeros((len(self.tokenizer.word_index) + 1,\n",
        "                                      self.hparams['embedding_dim']))\n",
        "    num_words_in_embedding = 0\n",
        "    for word, i in self.tokenizer.word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "        num_words_in_embedding += 1\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        self.embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  def train(self, training_data_path, validation_data_path, text_column,\n",
        "            label_column, model_name):\n",
        "    self.model_name = model_name\n",
        "    self.save_hparams(model_name)\n",
        "\n",
        "    train_data = pd.read_csv(training_data_path)\n",
        "    valid_data = pd.read_csv(validation_data_path)\n",
        "    train_data[text_column]=train_data[text_column].astype(str)\n",
        "    valid_data[text_column]=valid_data[text_column].astype(str)\n",
        "\n",
        "\n",
        "    if(self.tokenize):\n",
        "      print('Fitting tokenizer...')\n",
        "      self.fit_and_save_tokenizer(train_data[text_column])\n",
        "      print('Tokenizer fitted!')\n",
        "\n",
        "    print('Preparing data...')\n",
        "    ##################\n",
        "    if(self.typeArch == \"cnn_attr\"):\n",
        "      target_words = []\n",
        "      train_sep =\"./wiki_test_sep.txt\"\n",
        "      val_sep = \"./wiki_dev_sep.txt\"\n",
        "      self.seq_len = 100\n",
        "      idx2word, word2idx = build_vocab(train_sep, 5, \"./vocab.txt\")\n",
        "      train_text, train_labels, attr_target = preprocess(train_sep, self.seq_len, word2idx, target_words=target_words, return_attr_target=True, target_words_to_token=False)\n",
        "      self.train_labels = train_labels\n",
        "      self.baseline = get_all_pad(self.seq_len, word2idx)\n",
        "      self.attr_target = attr_target\n",
        "      self.word2idx = word2idx\n",
        "      self.idx2word = idx2word\n",
        "      valid_text, valid_labels = preprocess(val_sep, self.seq_len, word2idx, target_words=target_words)\n",
        "\n",
        "    ##################\n",
        "    else:\n",
        "      train_text, train_labels = (self.prep_text(train_data[text_column]),\n",
        "                                  tf.keras.utils.to_categorical(\n",
        "                                      train_data[label_column]))\n",
        "      valid_text, valid_labels = (self.prep_text(valid_data[text_column]),\n",
        "                                tf.keras.utils.to_categorical(\n",
        "                                    valid_data[label_column]))\n",
        "\n",
        "    print('Data prepared!')\n",
        "\n",
        "    if(self.tokenize):\n",
        "      print('Loading embeddings...')\n",
        "      self.load_embeddings()\n",
        "      print('Embeddings loaded!')\n",
        "\n",
        "    print('Building model graph...')\n",
        "    self.build_model()\n",
        "    print('Training model...')\n",
        "\n",
        "    save_path = os.path.join(self.model_dir, '%s_model.h5' % self.model_name)\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            save_path, save_best_only=True, verbose=self.hparams['verbose'])\n",
        "    ]\n",
        "\n",
        "    if self.hparams['stop_early']:\n",
        "      callbacks.append(\n",
        "          tf.keras.callbacks.EarlyStopping(\n",
        "              min_delta=self.hparams['es_min_delta'],\n",
        "              monitor='val_loss',\n",
        "              patience=self.hparams['es_patience'],\n",
        "              verbose=self.hparams['verbose'],\n",
        "              mode='auto'))\n",
        "\n",
        "    self.model.fit(\n",
        "        train_text,\n",
        "        train_labels,\n",
        "        batch_size=self.batch_size,\n",
        "        epochs=self.hparams['epochs'],\n",
        "        validation_data=(valid_text, valid_labels),\n",
        "        callbacks=callbacks,\n",
        "        verbose=2)\n",
        "    print('Model trained!')\n",
        "    print('Best model saved to {}'.format(save_path))\n",
        "    print('Loading best model from checkpoint...')\n",
        "    self.model = tf.keras.models.load_model(save_path)\n",
        "    print('Model loaded!')\n",
        "    return self.model\n",
        "\n",
        "  def train_shap(self, training_data_path, validation_data_path, text_column,\n",
        "            label_column, model_name):\n",
        "    train_data = pd.read_csv(training_data_path)\n",
        "    valid_data = pd.read_csv(validation_data_path)\n",
        "    train_data[text_column]=train_data[text_column].astype(str)\n",
        "    valid_data[text_column]=valid_data[text_column].astype(str)\n",
        "    self.fit_and_save_tokenizer(train_data[text_column])\n",
        "    x_train = train_data[text_column]\n",
        "    y_train = tf.keras.utils.to_categorical(\n",
        "                                      train_data[label_column])\n",
        "    x_test  = valid_data[text_column]\n",
        "    y_test = tf.keras.utils.to_categorical(\n",
        "                                      valid_data[label_column])\n",
        "\n",
        "\n",
        "    # text_sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "    maxlen = self.hparams['max_sequence_length']\n",
        "\n",
        "    x_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        self.tokenizer.texts_to_sequences(x_train), maxlen=self.hparams['max_sequence_length'])\n",
        "    x_test = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        self.tokenizer.texts_to_sequences(x_test), maxlen=self.hparams['max_sequence_length'])\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(20000, 128))\n",
        "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    rmsprop = tf.keras.optimizers.RMSprop(lr=self.hparams['learning_rate'])\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy', optimizer=rmsprop, metrics=['acc'])\n",
        "\n",
        "    print('Train...')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=128,\n",
        "              epochs=5,\n",
        "              validation_data=(x_test, y_test))\n",
        "    score, acc = model.evaluate(x_test, y_test,\n",
        "                                batch_size=128)\n",
        "    print('Test score:', score)\n",
        "    print('Test accuracy:', acc)\n",
        "    return model\n",
        "\n",
        "  def buildRNN(self):\n",
        "    sequence_input = tf.keras.layers.Input(\n",
        "        shape=(self.hparams['max_sequence_length'],), dtype='int32')\n",
        "    embedding_layer = tf.keras.layers.Embedding(\n",
        "        len(self.tokenizer.word_index) + 1,\n",
        "        self.hparams['embedding_dim'],\n",
        "        weights=[self.embedding_matrix],\n",
        "        input_length=self.hparams['max_sequence_length'],\n",
        "        trainable=self.hparams['embedding_trainable'])\n",
        "\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "    x = embedded_sequences\n",
        "    if(self.subtype == \"\"):\n",
        "      print(\"simple rnn\")\n",
        "      x = tf.keras.layers.SimpleRNN(128, input_shape=x.shape, return_sequences=False)(x)\n",
        "    elif(self.subtype == \"gru\"):\n",
        "      print(\"GRU\")\n",
        "      x = tf.keras.layers.GRU(128, input_shape=x.shape, return_sequences=False)(x)\n",
        "    elif(self.subtype == \"lstm\"):\n",
        "      print(\"LSTM\")\n",
        "      x = tf.keras.layers.LSTM(128, input_shape=x.shape, return_sequences=False)(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    preds = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
        "\n",
        "    rmsprop = tf.keras.optimizers.RMSprop(lr=self.hparams['learning_rate'])\n",
        "    self.model = tf.keras.Model(sequence_input, preds)\n",
        "    self.model.compile(\n",
        "        loss='categorical_crossentropy', optimizer=rmsprop, metrics=['acc'])\n",
        "\n",
        "\n",
        "  def buildCNN_attr(self):\n",
        "    \"\"\"Builds CNN ATTR model graph.\"\"\"\n",
        "    sequence_input = tf.keras.layers.Input(\n",
        "        shape=(self.seq_len,), dtype='int32')\n",
        "    # embedding_layer = tf.keras.layers.Embedding(\n",
        "    #     len(self.tokenizer.word_index) + 1,\n",
        "    #     self.hparams['embedding_dim'],\n",
        "    #     weights=[self.embedding_matrix],\n",
        "    #     input_length=self.hparams['max_sequence_length'],\n",
        "    #     trainable=self.hparams['embedding_trainable'])\n",
        "\n",
        "    # embedded_sequences = embedding_layer(sequence_input)\n",
        "    # x = embedded_sequences\n",
        "\n",
        "    ###PIG INTERPOLATION##########################################\n",
        "    embedding_dim = 128\n",
        "    init_embeddings = tf.random_uniform([len(self.idx2word),\n",
        "                                           embedding_dim])\n",
        "    x_emb = tf.nn.embedding_lookup(init_embeddings, sequence_input)\n",
        "    x_emb_expand = tf.expand_dims(x_emb, -1)\n",
        "    baseline_emb = tf.nn.embedding_lookup(init_embeddings, self.baseline)[0]\n",
        "\n",
        "    scaled_inp = []\n",
        "    self.steps = 50\n",
        "    for i in range(1, self.steps+1):\n",
        "      # print(\"baseline \", baseline_emb.shape, x.shape, sequence_input.shape)\n",
        "      scaled_inp.append((float(i) / self.steps) * (x_emb-baseline_emb) + baseline_emb)\n",
        "    inp_concat = tf.concat(scaled_inp, axis=0)\n",
        "    inp_concat_stop = tf.stop_gradient(inp_concat)\n",
        "    inp_concat_expand = tf.expand_dims(inp_concat_stop, -1)\n",
        "    # x = inp_concat_expand\n",
        "    ####END#####################################\n",
        "\n",
        "    # for filter_size, kernel_size, pool_size in zip(\n",
        "    #     self.hparams['cnn_filter_sizes'], self.hparams['cnn_kernel_sizes'],\n",
        "    #     self.hparams['cnn_pooling_sizes']):\n",
        "    #   x = self.build_conv_layer(x_emb_expand, filter_size, kernel_size, pool_size)\n",
        "\n",
        "    pooled_outputs = []\n",
        "    num_filters = 128\n",
        "    filter_sizes = [2,3,4]\n",
        "    pooled_outputs_scaled = []\n",
        "    for i, filter_size in enumerate(filter_sizes):\n",
        "      conv = tf.keras.layers.Conv2D(\n",
        "                              filters=num_filters,\n",
        "                              kernel_size=[filter_size, embedding_dim],\n",
        "                              strides=(1, 1),\n",
        "                              padding=\"VALID\",\n",
        "                              activation=tf.nn.relu,\n",
        "                              name='conv_'+ str(i))\n",
        "      conv = conv(x_emb_expand)\n",
        "      pool = tf.keras.layers.MaxPooling2D(\n",
        "                                      pool_size=[self.seq_len - filter_size + 1, 1],\n",
        "                                      strides=(1, 1),\n",
        "                                      padding=\"VALID\")(conv)\n",
        "      pooled_outputs.append(pool)\n",
        "\n",
        "\n",
        "      conv_scaled = tf.keras.layers.Conv2D(\n",
        "                                      filters=num_filters,\n",
        "                                      kernel_size=[filter_size, embedding_dim],\n",
        "                                      strides=(1, 1),\n",
        "                                      padding=\"VALID\",\n",
        "                                      activation=tf.nn.relu,\n",
        "                                      name='conv_'+ str(i))(inp_concat_expand)\n",
        "\n",
        "      pool_scaled = tf.keras.layers.MaxPooling2D(\n",
        "                                            pool_size=[self.seq_len - filter_size + 1, 1],\n",
        "                                            strides=(1, 1),\n",
        "                                            padding=\"VALID\")(conv_scaled)\n",
        "      pooled_outputs_scaled.append(pool_scaled)\n",
        "    h_pool = tf.concat(pooled_outputs, 3)\n",
        "    h_pool_scaled = tf.concat(pooled_outputs_scaled, 3)\n",
        "    self.h_pool_flat = tf.reshape(h_pool, [-1, num_filters * len(filter_sizes)])\n",
        "    self.h_pool_flat_scaled = tf.reshape(h_pool_scaled, [-1, num_filters * len(filter_sizes)])\n",
        "\n",
        "    ####START#########\n",
        "    drop_rate = 0.8\n",
        "    mask = tf.nn.dropout(tf.ones_like(self.h_pool_flat), drop_rate)\n",
        "    mask_scaled = tf.tile(mask, [self.steps, 1])\n",
        "    h_drop = self.h_pool_flat * mask\n",
        "    h_drop_scaled = self.h_pool_flat_scaled * mask_scaled\n",
        "\n",
        "    logits = tf.keras.layers.Dense(self.num_classes, activation=None)(h_drop)\n",
        "    logits_scaled = tf.keras.layers.Dense(self.num_classes, activation=None)(h_drop_scaled)\n",
        "    softmax = tf.nn.softmax(logits)\n",
        "    preds = softmax\n",
        "    softmax_scaled = tf.nn.softmax(logits_scaled)\n",
        "    ####END########\n",
        "\n",
        "\n",
        "    #####PIG############################################\n",
        "    self.grad = tf.gradients(softmax_scaled[:, 1], inp_concat_stop)[0]\n",
        "\n",
        "    grad_reshape = tf.reshape(self.grad, [self.steps, -1, self.seq_len, embedding_dim])\n",
        "    avg_grad = tf.reduce_mean(grad_reshape, axis=0)\n",
        "    intergrated_grad = avg_grad * (x_emb - baseline_emb)\n",
        "    self.tmp = intergrated_grad\n",
        "    self.sum_intergrated_grad = tf.reduce_sum(intergrated_grad, axis=2)\n",
        "    self.target_attribution = tf.multiply(self.sum_intergrated_grad, self.attr_target)\n",
        "\n",
        "    #######END##########################################\n",
        "\n",
        "    rmsprop = tf.keras.optimizers.Adam(learning_rate=self.hparams['learning_rate'])\n",
        "    print(\"seq/pred lengths\", sequence_input.shape, preds.shape)\n",
        "    self.model = tf.keras.Model(sequence_input, preds)\n",
        "\n",
        "\n",
        "    def myLoss(label, pred):\n",
        "      attr_loss_weight=100000000000\n",
        "      labels = tf.one_hot(tf.cast(label, tf.int32), depth=self.num_classes)\n",
        "      cross_entropy = - tf.reduce_sum(tf.multiply(labels, pred), reduction_indices=1)\n",
        "      clf_loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
        "      attr_loss = tf.losses.mean_squared_error(self.target_attribution, self.sum_intergrated_grad) * attr_loss_weight\n",
        "      print(\"Attr loss\", attr_loss.shape, clf_loss.shape)\n",
        "      return clf_loss + attr_loss\n",
        "    self.model.compile(\n",
        "        loss=myLoss, optimizer=rmsprop, metrics=['acc'], run_eagerly=self.eager)\n",
        "\n",
        "  def buildCNN(self):\n",
        "    \"\"\"Builds model graph.\"\"\"\n",
        "    sequence_input = tf.keras.layers.Input(\n",
        "        shape=(self.hparams['max_sequence_length'],), dtype='int32')\n",
        "    embedding_layer = tf.keras.layers.Embedding(\n",
        "        len(self.tokenizer.word_index) + 1,\n",
        "        self.hparams['embedding_dim'],\n",
        "        weights=[self.embedding_matrix],\n",
        "        input_length=self.hparams['max_sequence_length'],\n",
        "        trainable=self.hparams['embedding_trainable'])\n",
        "\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "    x = embedded_sequences\n",
        "    for filter_size, kernel_size, pool_size in zip(\n",
        "        self.hparams['cnn_filter_sizes'], self.hparams['cnn_kernel_sizes'],\n",
        "        self.hparams['cnn_pooling_sizes']):\n",
        "      x = self.build_conv_layer(x, filter_size, kernel_size, pool_size)\n",
        "\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dropout(self.hparams['dropout_rate'])(x)\n",
        "    # TODO(nthain): Parametrize the number and size of fully connected layers\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    preds = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
        "\n",
        "    rmsprop = tf.keras.optimizers.RMSprop(lr=self.hparams['learning_rate'])\n",
        "    print(\"seq/pred lengths\", sequence_input.shape, preds.shape)\n",
        "    self.model = tf.keras.Model(sequence_input, preds)\n",
        "    self.model.compile(\n",
        "        loss='categorical_crossentropy', optimizer=rmsprop, metrics=['acc'])\n",
        "\n",
        "  def build_model(self):\n",
        "    if(self.typeArch == \"cnn\"):\n",
        "      print(\"USING\", \"CNN\" )\n",
        "      self.buildCNN()\n",
        "    elif(self.typeArch == \"rnn\"):\n",
        "      print(\"USING\", \"RNN\" )\n",
        "      self.buildRNN()\n",
        "    elif(self.typeArch == \"cnn_attr\"):\n",
        "      print(\"USING\", \"CNN ATTR\" )\n",
        "      # tf.compat.v1.disable_eager_execution()\n",
        "      self.buildCNN_attr()\n",
        "      # tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "  def build_conv_layer(self, input_tensor, filter_size, kernel_size, pool_size):\n",
        "    output = tf.keras.layers.Conv1D(\n",
        "        filter_size, kernel_size, activation='relu', padding='same')(\n",
        "            input_tensor)\n",
        "    if pool_size:\n",
        "      output = tf.keras.layers.MaxPool1D(pool_size, padding='same')(output)\n",
        "    else:\n",
        "      output = tf.keras.layers.GlobalMaxPool1D()(output)\n",
        "    return output\n",
        "\n",
        "  def predict(self, texts):\n",
        "    \"\"\"Returns model predictions on texts.\"\"\"\n",
        "    data = self.prep_text(texts)\n",
        "    return self.model.predict(data)[:, 1]\n",
        "\n",
        "  def score_auc(self, texts, labels):\n",
        "    preds = self.predict(texts)\n",
        "    return compute_auc(labels, preds)\n",
        "\n",
        "  def summary(self):\n",
        "    return self.model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AeVwy0UQxCm"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import pandas as pd\n",
        "hparams = {'epochs': 4}\n",
        "SPLITS = ['train', 'dev', 'test']\n",
        "\n",
        "wiki = {}\n",
        "# debias = {}\n",
        "# random = {}\n",
        "noisy = {}\n",
        "for split in SPLITS:\n",
        "    wiki[split] = './wiki_%s.csv' % split\n",
        "    noisy[split] = './wiki_noised_%s.csv' % split\n",
        "    # random[split] = '../data/wiki_debias_random_%s.csv' % split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "id": "Gi3BUiR7RCFS",
        "outputId": "c51d3e1e-d9cb-4481-f170-6400d25c82f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters\n",
            "---------------\n",
            "max_sequence_length: 250\n",
            "max_num_words: 10000\n",
            "embedding_dim: 100\n",
            "seq_len: 10\n",
            "embedding_trainable: False\n",
            "learning_rate: 5e-05\n",
            "stop_early: True\n",
            "es_patience: 1\n",
            "es_min_delta: 0\n",
            "batch_size: 128\n",
            "epochs: 4\n",
            "dropout_rate: 0.3\n",
            "cnn_filter_sizes: [128, 128, 128]\n",
            "cnn_kernel_sizes: [5, 5, 5]\n",
            "cnn_pooling_sizes: [5, 5, 40]\n",
            "verbose: True\n",
            "\n",
            "Fitting tokenizer...\n",
            "Tokenizer fitted!\n",
            "Preparing data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f0fedd5c9b3e>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cnn_wiki_noised_tox_v3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mwiki_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mToxModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcnn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwiki_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypeData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypeData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'comment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'is_toxic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-5f9b6d3288cc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data_path, validation_data_path, text_column, label_column, model_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;31m##################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m       train_text, train_labels = (self.prep_text(train_data[text_column]),\n\u001b[0m\u001b[1;32m    236\u001b[0m                                   tf.keras.utils.to_categorical(\n\u001b[1;32m    237\u001b[0m                                       train_data[label_column]))\n",
            "\u001b[0;32m<ipython-input-5-5f9b6d3288cc>\u001b[0m in \u001b[0;36mprep_text\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mtokenized\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpadded\u001b[0m \u001b[0mtext\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \"\"\"\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0mtext_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     return tf.keras.preprocessing.sequence.pad_sequences(\n\u001b[1;32m    179\u001b[0m         text_sequences, maxlen=self.hparams['max_sequence_length'])\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_sequences\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \"\"\"\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_sequences_generator\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                     seq = text_to_word_sequence(\n\u001b[0m\u001b[1;32m    387\u001b[0m                         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                         \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mtranslate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mtranslate_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# MODEL_NAME = 'cnn_wiki_tox_v3'\n",
        "# wiki_model = ToxModel(hparams=hparams)\n",
        "# wiki_model.train(wiki['train'], wiki['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)\n",
        "typeData = noisy\n",
        "MODEL_NAME = 'cnn_wiki_noised_tox_v3'\n",
        "wiki_model = ToxModel(hparams=hparams)\n",
        "cnn_model = wiki_model.train(typeData['train'], typeData['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL_NAME = 'cnn_wiki_tox_v3'\n",
        "# wiki_model = ToxModel(hparams=hparams)\n",
        "# wiki_model.train(wiki['train'], wiki['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)\n",
        "typeData = noisy\n",
        "MODEL_NAME = 'shap_wiki_noised_tox_v3'\n",
        "wiki_model = ToxModel(hparams=hparams)\n",
        "shap_model = wiki_model.train_shap(typeData['train'], typeData['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWI5VMZ-h5DI",
        "outputId": "93876e54-0187-4317-d38e-223b5c359269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters\n",
            "---------------\n",
            "max_sequence_length: 250\n",
            "max_num_words: 10000\n",
            "embedding_dim: 100\n",
            "seq_len: 10\n",
            "embedding_trainable: False\n",
            "learning_rate: 5e-05\n",
            "stop_early: True\n",
            "es_patience: 1\n",
            "es_min_delta: 0\n",
            "batch_size: 128\n",
            "epochs: 4\n",
            "dropout_rate: 0.3\n",
            "cnn_filter_sizes: [128, 128, 128]\n",
            "cnn_kernel_sizes: [5, 5, 5]\n",
            "cnn_pooling_sizes: [5, 5, 40]\n",
            "verbose: True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/rmsprop.py:144: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train...\n",
            "Epoch 1/5\n",
            "748/748 [==============================] - 917s 1s/step - loss: 0.3243 - acc: 0.9024 - val_loss: 0.2638 - val_acc: 0.9051\n",
            "Epoch 2/5\n",
            "748/748 [==============================] - 920s 1s/step - loss: 0.2342 - acc: 0.9084 - val_loss: 0.2156 - val_acc: 0.9188\n",
            "Epoch 3/5\n",
            "748/748 [==============================] - 919s 1s/step - loss: 0.2014 - acc: 0.9224 - val_loss: 0.1948 - val_acc: 0.9273\n",
            "Epoch 4/5\n",
            "748/748 [==============================] - 928s 1s/step - loss: 0.1843 - acc: 0.9320 - val_loss: 0.1826 - val_acc: 0.9334\n",
            "Epoch 5/5\n",
            "748/748 [==============================] - 913s 1s/step - loss: 0.1727 - acc: 0.9374 - val_loss: 0.1778 - val_acc: 0.9383\n",
            "251/251 [==============================] - 19s 75ms/step - loss: 0.1778 - acc: 0.9383\n",
            "Test score: 0.17781531810760498\n",
            "Test accuracy: 0.9383092522621155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./shap.pkl', 'wb') as f:\n",
        "    pickle.dump(shap_model, f)"
      ],
      "metadata": {
        "id": "VUMqFOYwKhB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(typeData['train'])\n",
        "valid_data = pd.read_csv(typeData['dev'])\n",
        "train_data['comment']=train_data['comment'].astype(str)\n",
        "valid_data['comment']=valid_data['comment'].astype(str)\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=10000)\n",
        "tokenizer.fit_on_texts(train_data['comment'])\n",
        "\n",
        "x_train = train_data['comment']\n",
        "y_train = tf.keras.utils.to_categorical(\n",
        "                                  train_data['is_toxic'])\n",
        "x_test  = valid_data['comment']\n",
        "y_test = tf.keras.utils.to_categorical(\n",
        "                                  valid_data['is_toxic'])\n",
        "\n",
        "\n",
        "# text_sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    tokenizer.texts_to_sequences(x_train), maxlen=250)\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    tokenizer.texts_to_sequences(x_test), maxlen=250)"
      ],
      "metadata": {
        "id": "RW-8C5z3QaSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shap\n",
        "\n",
        "# select a set of background examples to take an expectation over\n",
        "# x_train, y_train, x_test, y_test = m.getData(typeData['train'], typeData['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)\n",
        "# we use the first 100 training examples as our background dataset to integrate over\n",
        "explainer = shap.DeepExplainer(shap_model, x_train[:100])\n",
        "\n",
        "# explain the first 10 predictions\n",
        "# explaining each prediction requires 2 * background dataset size runs\n",
        "print(x_test[:10])\n",
        "shap_values = explainer.shap_values(x_train[:10])\n",
        "# init the JS visualization code\n",
        "shap.initjs()\n",
        "\n",
        "# transform the indexes to words\n",
        "import numpy as np\n",
        "words = imdb.get_word_index()\n",
        "num2word = {}\n",
        "for w in words.keys():\n",
        "    num2word[words[w]] = w\n",
        "x_test_words = np.stack([np.array(list(map(lambda x: num2word.get(x, \"NONE\"), x_test[i]))) for i in range(10)])\n",
        "\n",
        "# plot the explanation of the first prediction\n",
        "# Note the model is \"multi-output\" because it is rank-2 but only has one column\n",
        "shap.force_plot(explainer.expected_value[0], shap_values[0][0], x_test_words[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "id": "XnKOT1J8QRZH",
        "outputId": "b74733f3-2fa3-4ef9-a14b-43c733ce842f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   1 1473   29 ... 2399   33    1]\n",
            " [ 614    1    2 ...    1    1    1]\n",
            " [   0    0    0 ...  294 1077    1]\n",
            " ...\n",
            " [   0    0    0 ... 1414   13   21]\n",
            " [   0    0    0 ...    1  439    6]\n",
            " [   0    0    0 ...  113    1   16]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b359e3312c54>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# explaining each prediction requires 2 * background dataset size runs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# init the JS visualization code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitjs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shap/explainers/_deep/__init__.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mwere\u001b[0m \u001b[0mchosen\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m\"top\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranked_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_rank_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_additivity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_additivity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shap/explainers/_deep/deep_tf.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0;31m# run attribution computation graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfeature_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output_ranks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0msample_phis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphi_symbolic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;31m# assign the attributions to the right part of the output arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shap/explainers/_deep/deep_tf.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, out, model_inputs, X)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_with_overridden_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcustom_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shap/explainers/_deep/deep_tf.py\u001b[0m in \u001b[0;36mexecute_with_overridden_gradients\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# define the computation graph for the attribution values using a custom gradient-like computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;31m# reinstate the backpropagatable check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shap/explainers/_deep/deep_tf.py\u001b[0m in \u001b[0;36manon\u001b[0;34m()\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                     \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m                     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tINskQ8JRN4L"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'RNN_noised_wiki_tox'\n",
        "typeData = noisy\n",
        "wiki_model = ToxModel(hparams=hparams, typeArch=\"rnn\")\n",
        "wiki_model.train(typeData['train'], typeData['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut2H4NlyjKyW"
      },
      "outputs": [],
      "source": [
        "# MODEL_NAME = 'CNN-attr_wiki_tox'\n",
        "# wiki_model = ToxModel(hparams=hparams, typeArch=\"cnn_attr\")\n",
        "# wiki_model.train(wiki['train'], wiki['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWd0fr9n0bTz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4741eb-f0fb-4552-8151-83d6b849988c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters\n",
            "---------------\n",
            "max_sequence_length: 250\n",
            "max_num_words: 10000\n",
            "embedding_dim: 100\n",
            "seq_len: 10\n",
            "embedding_trainable: False\n",
            "learning_rate: 5e-05\n",
            "stop_early: True\n",
            "es_patience: 1\n",
            "es_min_delta: 0\n",
            "batch_size: 128\n",
            "epochs: 4\n",
            "dropout_rate: 0.3\n",
            "cnn_filter_sizes: [128, 128, 128]\n",
            "cnn_kernel_sizes: [5, 5, 5]\n",
            "cnn_pooling_sizes: [5, 5, 40]\n",
            "verbose: True\n",
            "\n",
            "Fitting tokenizer...\n",
            "Tokenizer fitted!\n",
            "Preparing data...\n",
            "Data prepared!\n",
            "Loading embeddings...\n",
            "Embeddings loaded!\n",
            "Building model graph...\n",
            "USING RNN\n",
            "LSTM\n",
            "Training model...\n",
            "Epoch 1/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/rmsprop.py:144: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 0.18536, saving model to ./RNN-LSTM_wiki_tox_model.h5\n",
            "748/748 - 1179s - loss: 0.2259 - acc: 0.9231 - val_loss: 0.1854 - val_acc: 0.9317 - 1179s/epoch - 2s/step\n",
            "Epoch 2/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: val_loss improved from 0.18536 to 0.16700, saving model to ./RNN-LSTM_wiki_tox_model.h5\n",
            "748/748 - 1268s - loss: 0.1695 - acc: 0.9387 - val_loss: 0.1670 - val_acc: 0.9411 - 1268s/epoch - 2s/step\n",
            "Epoch 3/4\n",
            "\n",
            "Epoch 3: val_loss improved from 0.16700 to 0.15653, saving model to ./RNN-LSTM_wiki_tox_model.h5\n",
            "748/748 - 1207s - loss: 0.1585 - acc: 0.9424 - val_loss: 0.1565 - val_acc: 0.9441 - 1207s/epoch - 2s/step\n",
            "Epoch 4/4\n",
            "\n",
            "Epoch 4: val_loss improved from 0.15653 to 0.14984, saving model to ./RNN-LSTM_wiki_tox_model.h5\n",
            "748/748 - 1221s - loss: 0.1524 - acc: 0.9442 - val_loss: 0.1498 - val_acc: 0.9442 - 1221s/epoch - 2s/step\n",
            "Model trained!\n",
            "Best model saved to ./RNN-LSTM_wiki_tox_model.h5\n",
            "Loading best model from checkpoint...\n",
            "Model loaded!\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = 'RNN-LSTM_wiki_tox'\n",
        "typeData = wiki\n",
        "wiki_model = ToxModel(hparams=hparams, typeArch=\"rnn\", subtype=\"lstm\")\n",
        "wiki_model.train(typeData['train'], typeData['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'RNN-LSTM_noisy_wiki_tox'\n",
        "typeData = noisy\n",
        "wiki_model = ToxModel(hparams=hparams, typeArch=\"rnn\", subtype=\"lstm\")\n",
        "wiki_model.train(typeData['train'], typeData['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bS7RyyfP_Hx",
        "outputId": "9e847c19-7206-415e-b273-24821e63d370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters\n",
            "---------------\n",
            "max_sequence_length: 250\n",
            "max_num_words: 10000\n",
            "embedding_dim: 100\n",
            "seq_len: 10\n",
            "embedding_trainable: False\n",
            "learning_rate: 5e-05\n",
            "stop_early: True\n",
            "es_patience: 1\n",
            "es_min_delta: 0\n",
            "batch_size: 128\n",
            "epochs: 4\n",
            "dropout_rate: 0.3\n",
            "cnn_filter_sizes: [128, 128, 128]\n",
            "cnn_kernel_sizes: [5, 5, 5]\n",
            "cnn_pooling_sizes: [5, 5, 40]\n",
            "verbose: True\n",
            "\n",
            "Fitting tokenizer...\n",
            "Tokenizer fitted!\n",
            "Preparing data...\n",
            "Data prepared!\n",
            "Loading embeddings...\n",
            "Embeddings loaded!\n",
            "Building model graph...\n",
            "USING RNN\n",
            "LSTM\n",
            "Training model...\n",
            "Epoch 1/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/rmsprop.py:144: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 0.20581, saving model to ./RNN-LSTM_noisy_wiki_tox_model.h5\n",
            "748/748 - 825s - loss: 0.2533 - acc: 0.9150 - val_loss: 0.2058 - val_acc: 0.9271 - 825s/epoch - 1s/step\n",
            "Epoch 2/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: val_loss improved from 0.20581 to 0.18868, saving model to ./RNN-LSTM_noisy_wiki_tox_model.h5\n",
            "748/748 - 760s - loss: 0.1992 - acc: 0.9292 - val_loss: 0.1887 - val_acc: 0.9344 - 760s/epoch - 1s/step\n",
            "Epoch 3/4\n",
            "\n",
            "Epoch 3: val_loss improved from 0.18868 to 0.18266, saving model to ./RNN-LSTM_noisy_wiki_tox_model.h5\n",
            "748/748 - 757s - loss: 0.1894 - acc: 0.9321 - val_loss: 0.1827 - val_acc: 0.9356 - 757s/epoch - 1s/step\n",
            "Epoch 4/4\n",
            "\n",
            "Epoch 4: val_loss improved from 0.18266 to 0.17985, saving model to ./RNN-LSTM_noisy_wiki_tox_model.h5\n",
            "748/748 - 778s - loss: 0.1842 - acc: 0.9343 - val_loss: 0.1799 - val_acc: 0.9362 - 778s/epoch - 1s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained!\n",
            "Best model saved to ./RNN-LSTM_noisy_wiki_tox_model.h5\n",
            "Loading best model from checkpoint...\n",
            "Model loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'CNN_wiki_tox'\n",
        "typeData = wiki\n",
        "wiki_model = ToxModel(hparams=hparams, typeArch=\"cnn\")\n",
        "wiki_model.train(typeData['train'], typeData['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF6ocUYBm6Zk",
        "outputId": "d4d6b40d-a707-4c60-9520-a270723db068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters\n",
            "---------------\n",
            "max_sequence_length: 250\n",
            "max_num_words: 10000\n",
            "embedding_dim: 100\n",
            "seq_len: 10\n",
            "embedding_trainable: False\n",
            "learning_rate: 5e-05\n",
            "stop_early: True\n",
            "es_patience: 1\n",
            "es_min_delta: 0\n",
            "batch_size: 128\n",
            "epochs: 4\n",
            "dropout_rate: 0.3\n",
            "cnn_filter_sizes: [128, 128, 128]\n",
            "cnn_kernel_sizes: [5, 5, 5]\n",
            "cnn_pooling_sizes: [5, 5, 40]\n",
            "verbose: True\n",
            "\n",
            "Fitting tokenizer...\n",
            "Tokenizer fitted!\n",
            "Preparing data...\n",
            "Data prepared!\n",
            "Loading embeddings...\n",
            "Embeddings loaded!\n",
            "Building model graph...\n",
            "USING CNN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/rmsprop.py:144: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seq/pred lengths (None, 250) (None, 2)\n",
            "Training model...\n",
            "Epoch 1/4\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 0.17557, saving model to ./CNN_wiki_tox_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "748/748 - 15s - loss: 0.2383 - acc: 0.9154 - val_loss: 0.1756 - val_acc: 0.9361 - 15s/epoch - 20ms/step\n",
            "Epoch 2/4\n",
            "\n",
            "Epoch 2: val_loss improved from 0.17557 to 0.15794, saving model to ./CNN_wiki_tox_model.h5\n",
            "748/748 - 8s - loss: 0.1681 - acc: 0.9380 - val_loss: 0.1579 - val_acc: 0.9426 - 8s/epoch - 10ms/step\n",
            "Epoch 3/4\n",
            "\n",
            "Epoch 3: val_loss improved from 0.15794 to 0.14523, saving model to ./CNN_wiki_tox_model.h5\n",
            "748/748 - 7s - loss: 0.1495 - acc: 0.9441 - val_loss: 0.1452 - val_acc: 0.9471 - 7s/epoch - 10ms/step\n",
            "Epoch 4/4\n",
            "\n",
            "Epoch 4: val_loss did not improve from 0.14523\n",
            "748/748 - 8s - loss: 0.1373 - acc: 0.9490 - val_loss: 0.1476 - val_acc: 0.9436 - 8s/epoch - 10ms/step\n",
            "Epoch 4: early stopping\n",
            "Model trained!\n",
            "Best model saved to ./CNN_wiki_tox_model.h5\n",
            "Loading best model from checkpoint...\n",
            "Model loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'RNN-GRU_noisy_wiki_tox'\n",
        "typeData = noisy\n",
        "wiki_model = ToxModel(hparams=hparams, typeArch=\"rnn\", subtype=\"gru\")\n",
        "wiki_model.train(typeData['train'], typeData['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUq3PI7lnAZW",
        "outputId": "d6d2768c-892b-48e2-8e18-0c90385559e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters\n",
            "---------------\n",
            "max_sequence_length: 250\n",
            "max_num_words: 10000\n",
            "embedding_dim: 100\n",
            "seq_len: 10\n",
            "embedding_trainable: False\n",
            "learning_rate: 5e-05\n",
            "stop_early: True\n",
            "es_patience: 1\n",
            "es_min_delta: 0\n",
            "batch_size: 128\n",
            "epochs: 4\n",
            "dropout_rate: 0.3\n",
            "cnn_filter_sizes: [128, 128, 128]\n",
            "cnn_kernel_sizes: [5, 5, 5]\n",
            "cnn_pooling_sizes: [5, 5, 40]\n",
            "verbose: True\n",
            "\n",
            "Fitting tokenizer...\n",
            "Tokenizer fitted!\n",
            "Preparing data...\n",
            "Data prepared!\n",
            "Loading embeddings...\n",
            "Embeddings loaded!\n",
            "Building model graph...\n",
            "USING RNN\n",
            "GRU\n",
            "Training model...\n",
            "Epoch 1/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/rmsprop.py:144: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 0.21206, saving model to ./RNN-GRU_noisy_wiki_tox_model.h5\n",
            "748/748 - 621s - loss: 0.2735 - acc: 0.9073 - val_loss: 0.2121 - val_acc: 0.9296 - 621s/epoch - 831ms/step\n",
            "Epoch 2/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: val_loss improved from 0.21206 to 0.18744, saving model to ./RNN-GRU_noisy_wiki_tox_model.h5\n",
            "748/748 - 627s - loss: 0.1969 - acc: 0.9303 - val_loss: 0.1874 - val_acc: 0.9347 - 627s/epoch - 838ms/step\n",
            "Epoch 3/4\n",
            "\n",
            "Epoch 3: val_loss did not improve from 0.18744\n",
            "748/748 - 639s - loss: 0.1859 - acc: 0.9335 - val_loss: 0.1895 - val_acc: 0.9327 - 639s/epoch - 855ms/step\n",
            "Epoch 3: early stopping\n",
            "Model trained!\n",
            "Best model saved to ./RNN-GRU_noisy_wiki_tox_model.h5\n",
            "Loading best model from checkpoint...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'RNN-GRU_wiki_tox'\n",
        "typeData = wiki\n",
        "wiki_model = ToxModel(hparams=hparams, typeArch=\"rnn\", subtype=\"gru\")\n",
        "wiki_model.train(typeData['train'], typeData['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ7SH00qnFPF",
        "outputId": "417c5386-5d42-4e7d-b0e5-714d14c8c3b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters\n",
            "---------------\n",
            "max_sequence_length: 250\n",
            "max_num_words: 10000\n",
            "embedding_dim: 100\n",
            "seq_len: 10\n",
            "embedding_trainable: False\n",
            "learning_rate: 5e-05\n",
            "stop_early: True\n",
            "es_patience: 1\n",
            "es_min_delta: 0\n",
            "batch_size: 128\n",
            "epochs: 4\n",
            "dropout_rate: 0.3\n",
            "cnn_filter_sizes: [128, 128, 128]\n",
            "cnn_kernel_sizes: [5, 5, 5]\n",
            "cnn_pooling_sizes: [5, 5, 40]\n",
            "verbose: True\n",
            "\n",
            "Fitting tokenizer...\n",
            "Tokenizer fitted!\n",
            "Preparing data...\n",
            "Data prepared!\n",
            "Loading embeddings...\n",
            "Embeddings loaded!\n",
            "Building model graph...\n",
            "USING RNN\n",
            "GRU\n",
            "Training model...\n",
            "Epoch 1/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/rmsprop.py:144: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 0.18369, saving model to ./RNN-GRU_wiki_tox_model.h5\n",
            "748/748 - 621s - loss: 0.2405 - acc: 0.9213 - val_loss: 0.1837 - val_acc: 0.9362 - 621s/epoch - 830ms/step\n",
            "Epoch 2/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: val_loss improved from 0.18369 to 0.15777, saving model to ./RNN-GRU_wiki_tox_model.h5\n",
            "748/748 - 637s - loss: 0.1707 - acc: 0.9395 - val_loss: 0.1578 - val_acc: 0.9424 - 637s/epoch - 852ms/step\n",
            "Epoch 3/4\n",
            "\n",
            "Epoch 3: val_loss improved from 0.15777 to 0.14881, saving model to ./RNN-GRU_wiki_tox_model.h5\n",
            "748/748 - 646s - loss: 0.1568 - acc: 0.9431 - val_loss: 0.1488 - val_acc: 0.9450 - 646s/epoch - 864ms/step\n",
            "Epoch 4/4\n",
            "\n",
            "Epoch 4: val_loss improved from 0.14881 to 0.14495, saving model to ./RNN-GRU_wiki_tox_model.h5\n",
            "748/748 - 623s - loss: 0.1487 - acc: 0.9456 - val_loss: 0.1449 - val_acc: 0.9463 - 623s/epoch - 833ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained!\n",
            "Best model saved to ./RNN-GRU_wiki_tox_model.h5\n",
            "Loading best model from checkpoint...\n",
            "Model loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5pe0evalnJqU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}